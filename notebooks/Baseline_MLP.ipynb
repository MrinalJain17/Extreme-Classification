{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12409 examples in training set, and 3102 in validation.\n"
     ]
    }
   ],
   "source": [
    "# Needed to import custom code from other directories\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils import LRAP\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "NUM_FEATURES = 5000\n",
    "NUM_CLASSES = 3993\n",
    "\n",
    "X_train = pd.read_csv(\"../data/expanded/train_features.csv\", names=range(NUM_FEATURES))\n",
    "y_train = pd.read_csv(\"../data/expanded/train_labels.csv\", names=range(NUM_CLASSES))\n",
    "\n",
    "X_valid = pd.read_csv(\"../data/expanded/valid_features.csv\", names=range(NUM_FEATURES))\n",
    "y_valid = pd.read_csv(\"../data/expanded/valid_labels.csv\", names=range(NUM_CLASSES))\n",
    "\n",
    "print(f\"{X_train.shape[0]} examples in training set, and {X_valid.shape[0]} in validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "X_train_standardized = scaler.transform(X_train)\n",
    "X_valid_standardized = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 824.43684365\n",
      "Iteration 2, loss = 276.27994736\n",
      "Iteration 3, loss = 50.86958827\n",
      "Iteration 4, loss = 41.01838049\n",
      "Iteration 5, loss = 37.47571328\n",
      "Iteration 6, loss = 35.34513904\n",
      "Iteration 7, loss = 33.74758310\n",
      "Iteration 8, loss = 32.41534321\n",
      "Iteration 9, loss = 31.32393077\n",
      "Iteration 10, loss = 30.33921156\n",
      "Iteration 11, loss = 29.44053214\n",
      "Iteration 12, loss = 28.59708911\n",
      "Iteration 13, loss = 27.80045856\n",
      "Iteration 14, loss = 27.06895245\n",
      "Iteration 15, loss = 26.35433190\n",
      "Iteration 16, loss = 25.65371262\n",
      "Iteration 17, loss = 24.99835283\n",
      "Iteration 18, loss = 24.36534995\n",
      "Iteration 19, loss = 23.76539986\n",
      "Iteration 20, loss = 23.18088344\n",
      "Iteration 21, loss = 22.61777803\n",
      "Iteration 22, loss = 22.04537298\n",
      "Iteration 23, loss = 21.50852866\n",
      "Iteration 24, loss = 20.98532227\n",
      "Iteration 25, loss = 20.47608806\n",
      "Iteration 26, loss = 19.98254618\n",
      "Iteration 27, loss = 19.49883112\n",
      "Iteration 28, loss = 19.02146132\n",
      "Iteration 29, loss = 18.55018912\n",
      "Iteration 30, loss = 18.10077871\n",
      "Iteration 31, loss = 17.67435186\n",
      "Iteration 32, loss = 17.22958540\n",
      "Iteration 33, loss = 16.80328163\n",
      "Iteration 34, loss = 16.38673333\n",
      "Iteration 35, loss = inf\n",
      "Iteration 36, loss = 15.59826295\n",
      "Iteration 37, loss = 15.21483336\n",
      "Iteration 38, loss = 14.84675675\n",
      "Iteration 39, loss = 14.48942397\n",
      "Iteration 40, loss = 14.11468375\n",
      "Iteration 41, loss = 13.77333093\n",
      "Iteration 42, loss = 13.42509801\n",
      "Iteration 43, loss = 13.10115858\n",
      "Iteration 44, loss = 12.76052211\n",
      "Iteration 45, loss = 12.42786205\n",
      "Iteration 46, loss = 12.12558200\n",
      "Iteration 47, loss = 11.81528260\n",
      "Iteration 48, loss = 11.54132909\n",
      "Iteration 49, loss = 11.24020725\n",
      "Iteration 50, loss = 10.95790980\n",
      "Iteration 51, loss = 10.67583729\n",
      "Iteration 52, loss = 10.41479676\n",
      "Iteration 53, loss = 10.12986872\n",
      "Iteration 54, loss = 9.88845410\n",
      "Iteration 55, loss = 9.63497838\n",
      "Iteration 56, loss = 9.38342469\n",
      "Iteration 57, loss = 9.15368776\n",
      "Iteration 58, loss = 8.92667499\n",
      "Iteration 59, loss = 8.73530040\n",
      "Iteration 60, loss = inf\n",
      "Iteration 61, loss = 8.28405413\n",
      "Iteration 62, loss = 8.08164771\n",
      "Iteration 63, loss = 7.86012615\n",
      "Iteration 64, loss = 7.65757672\n",
      "Iteration 65, loss = 7.50807343\n",
      "Iteration 66, loss = inf\n",
      "Iteration 67, loss = 7.12673920\n",
      "Iteration 68, loss = 6.98449923\n",
      "Iteration 69, loss = 6.79683851\n",
      "Iteration 70, loss = inf\n",
      "Iteration 71, loss = 6.44965692\n",
      "Iteration 72, loss = 6.29166720\n",
      "Iteration 73, loss = 6.16164496\n",
      "Iteration 74, loss = inf\n",
      "Iteration 75, loss = 5.83151601\n",
      "Iteration 76, loss = 5.67906736\n",
      "Iteration 77, loss = 5.55275117\n",
      "Iteration 78, loss = 5.40480310\n",
      "Iteration 79, loss = 5.30454472\n",
      "Iteration 80, loss = 5.17455648\n",
      "Iteration 81, loss = 5.03988942\n",
      "Iteration 82, loss = 4.91541846\n",
      "Iteration 83, loss = 4.82490616\n",
      "Iteration 84, loss = 4.78135152\n",
      "Iteration 85, loss = 4.65234923\n",
      "Iteration 86, loss = 4.53020527\n",
      "Iteration 87, loss = 4.36339289\n",
      "Iteration 88, loss = 4.30154969\n",
      "Iteration 89, loss = 4.24432270\n",
      "Iteration 90, loss = 4.11721118\n",
      "Iteration 91, loss = inf\n",
      "Iteration 92, loss = 3.93627716\n",
      "Iteration 93, loss = 3.81939463\n",
      "Iteration 94, loss = inf\n",
      "Iteration 95, loss = inf\n",
      "Iteration 96, loss = 3.61087530\n",
      "Iteration 97, loss = 3.50629430\n",
      "Iteration 98, loss = 3.41663133\n",
      "Iteration 99, loss = 3.37403270\n",
      "Iteration 100, loss = inf\n",
      "Iteration 101, loss = inf\n",
      "Iteration 102, loss = 3.15976157\n",
      "Iteration 103, loss = 3.08537007\n",
      "Iteration 104, loss = 3.01407320\n",
      "Iteration 105, loss = 2.90842132\n",
      "Iteration 106, loss = 2.83365599\n",
      "Iteration 107, loss = 2.80414203\n",
      "Iteration 108, loss = inf\n",
      "Iteration 109, loss = 2.67373099\n",
      "Iteration 110, loss = 2.65904556\n",
      "Iteration 111, loss = inf\n",
      "Iteration 112, loss = 2.49536813\n",
      "Iteration 113, loss = 2.46622013\n",
      "Iteration 114, loss = 2.46197683\n",
      "Iteration 115, loss = inf\n",
      "Iteration 116, loss = 2.45054764\n",
      "Iteration 117, loss = 2.44782074\n",
      "Iteration 118, loss = inf\n",
      "Iteration 119, loss = inf\n",
      "Iteration 120, loss = 2.31023274\n",
      "Iteration 121, loss = inf\n",
      "Iteration 122, loss = inf\n",
      "Iteration 123, loss = inf\n",
      "Iteration 124, loss = 2.07164987\n",
      "Iteration 125, loss = 1.98605373\n",
      "Iteration 126, loss = 1.90915983\n",
      "Iteration 127, loss = 1.86856830\n",
      "Iteration 128, loss = inf\n",
      "Iteration 129, loss = 1.81993421\n",
      "Iteration 130, loss = 1.81865617\n",
      "Iteration 131, loss = 1.72968983\n",
      "Iteration 132, loss = 1.71978120\n",
      "Iteration 133, loss = 1.68313135\n",
      "Iteration 134, loss = 1.64603133\n",
      "Iteration 135, loss = 1.68983221\n",
      "Iteration 136, loss = inf\n",
      "Iteration 137, loss = 1.55839920\n",
      "Iteration 138, loss = 1.57677204\n",
      "Iteration 139, loss = inf\n",
      "Iteration 140, loss = inf\n",
      "Iteration 141, loss = 1.53169158\n",
      "Iteration 142, loss = 1.52025477\n",
      "Iteration 143, loss = 1.54045521\n",
      "Iteration 144, loss = 1.55000848\n",
      "Iteration 145, loss = inf\n",
      "Iteration 146, loss = inf\n",
      "Iteration 147, loss = inf\n",
      "Iteration 148, loss = inf\n",
      "Iteration 149, loss = 1.48448586\n",
      "Iteration 150, loss = 1.41232961\n",
      "Iteration 151, loss = inf\n",
      "Iteration 152, loss = 1.31197913\n",
      "Iteration 153, loss = 1.22508722\n",
      "Iteration 154, loss = 1.21233674\n",
      "Iteration 155, loss = 1.16403714\n",
      "Iteration 156, loss = 1.17169911\n",
      "Iteration 157, loss = 1.15563799\n",
      "Iteration 158, loss = 1.15855217\n",
      "Iteration 159, loss = 1.16544504\n",
      "Iteration 160, loss = 1.10984232\n",
      "Iteration 161, loss = inf\n",
      "Iteration 162, loss = 1.07184698\n",
      "Iteration 163, loss = 1.09752723\n",
      "Iteration 164, loss = inf\n",
      "Iteration 165, loss = inf\n",
      "Iteration 166, loss = 1.03423050\n",
      "Iteration 167, loss = 1.02872478\n",
      "Iteration 168, loss = inf\n",
      "Iteration 169, loss = inf\n",
      "Iteration 170, loss = 0.96943204\n",
      "Iteration 171, loss = 0.94071169\n",
      "Iteration 172, loss = 0.95976417\n",
      "Iteration 173, loss = 0.93340477\n",
      "Iteration 174, loss = inf\n",
      "Iteration 175, loss = 0.95354816\n",
      "Iteration 176, loss = 0.98962093\n",
      "Iteration 177, loss = 0.95127719\n",
      "Iteration 178, loss = 1.02225956\n",
      "Iteration 179, loss = inf\n",
      "Iteration 180, loss = inf\n",
      "Iteration 181, loss = inf\n",
      "Iteration 182, loss = inf\n",
      "Iteration 183, loss = inf\n",
      "Iteration 184, loss = inf\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# Training takes ~12 seconds per epoch/iteration\n",
    "mlp = MLPClassifier(\n",
    "    verbose=True,\n",
    "    random_state=SEED\n",
    ")\n",
    "mlp = mlp.fit(X_train_standardized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRAP on training data: 0.9906\n"
     ]
    }
   ],
   "source": [
    "pred_train = mlp.predict_proba(X_train_standardized)\n",
    "score_train = LRAP(y_train, pred_train)\n",
    "print(f\"LRAP on training data: {score_train:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRAP on validation data: 0.4724\n"
     ]
    }
   ],
   "source": [
    "pred_valid = mlp.predict_proba(X_valid_standardized)\n",
    "score_valid = LRAP(y_valid, pred_valid)\n",
    "print(f\"LRAP on validation data: {score_valid:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
