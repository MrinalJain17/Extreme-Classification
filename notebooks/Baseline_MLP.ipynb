{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to import custom code from other directories\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils import split\n",
    "from utils import LRAP\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "NUM_FEATURES = 5000\n",
    "NUM_CLASSES = 3993\n",
    "\n",
    "features = pd.read_csv(\"../data/expanded/train_features.csv\", names=range(NUM_FEATURES))\n",
    "labels = pd.read_csv(\"../data/expanded/train_labels.csv\", names=range(NUM_CLASSES))\n",
    "\n",
    "# Splitting the data: 10% validation, 90% training (by default)\n",
    "X_train, X_valid, y_train, y_valid = split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "X_train_standardized = scaler.transform(X_train)\n",
    "X_valid_standardized = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 848.22752478\n",
      "Iteration 2, loss = 197.14396933\n",
      "Iteration 3, loss = 88.68520050\n",
      "Iteration 4, loss = 60.73337127\n",
      "Iteration 5, loss = 49.73010257\n",
      "Iteration 6, loss = 44.71638846\n",
      "Iteration 7, loss = 41.81444310\n",
      "Iteration 8, loss = 39.94563209\n",
      "Iteration 9, loss = 38.57930430\n",
      "Iteration 10, loss = 37.46411350\n",
      "Iteration 11, loss = 36.57145257\n",
      "Iteration 12, loss = 35.90000950\n",
      "Iteration 13, loss = 35.38897527\n",
      "Iteration 14, loss = 34.91621125\n",
      "Iteration 15, loss = 34.47343275\n",
      "Iteration 16, loss = 34.05196159\n",
      "Iteration 17, loss = 33.66802953\n",
      "Iteration 18, loss = 33.30140912\n",
      "Iteration 19, loss = 32.92648340\n",
      "Iteration 20, loss = 32.58014430\n",
      "Iteration 21, loss = 32.23613773\n",
      "Iteration 22, loss = 31.89688152\n",
      "Iteration 23, loss = 31.57116410\n",
      "Iteration 24, loss = 31.24182078\n",
      "Iteration 25, loss = 30.91978036\n",
      "Iteration 26, loss = 30.59089837\n",
      "Iteration 27, loss = 30.27938861\n",
      "Iteration 28, loss = 29.97425031\n",
      "Iteration 29, loss = 29.67160362\n",
      "Iteration 30, loss = 29.36820769\n",
      "Iteration 31, loss = 29.06562756\n",
      "Iteration 32, loss = 28.76855723\n",
      "Iteration 33, loss = 28.47369132\n",
      "Iteration 34, loss = 28.18003236\n",
      "Iteration 35, loss = 27.89131338\n",
      "Iteration 36, loss = 27.60057299\n",
      "Iteration 37, loss = 27.31615232\n",
      "Iteration 38, loss = 27.03177903\n",
      "Iteration 39, loss = 26.74591856\n",
      "Iteration 40, loss = 26.46315653\n",
      "Iteration 41, loss = 26.18156000\n",
      "Iteration 42, loss = 25.90061263\n",
      "Iteration 43, loss = 25.61618290\n",
      "Iteration 44, loss = 25.33266139\n",
      "Iteration 45, loss = 25.05307575\n",
      "Iteration 46, loss = 24.77077835\n",
      "Iteration 47, loss = 24.49183053\n",
      "Iteration 48, loss = 24.21306279\n",
      "Iteration 49, loss = 23.93329441\n",
      "Iteration 50, loss = 23.65700864\n",
      "Iteration 51, loss = 23.38160247\n",
      "Iteration 52, loss = 23.10703976\n",
      "Iteration 53, loss = 22.83399090\n",
      "Iteration 54, loss = 22.55940635\n",
      "Iteration 55, loss = 22.28720661\n",
      "Iteration 56, loss = 22.01467467\n",
      "Iteration 57, loss = 21.74697205\n",
      "Iteration 58, loss = 21.47460201\n",
      "Iteration 59, loss = 21.20688212\n",
      "Iteration 60, loss = 20.94155935\n",
      "Iteration 61, loss = 20.67330727\n",
      "Iteration 62, loss = 20.41145321\n",
      "Iteration 63, loss = 20.14534867\n",
      "Iteration 64, loss = 19.87880918\n",
      "Iteration 65, loss = 19.62265461\n",
      "Iteration 66, loss = 19.35898978\n",
      "Iteration 67, loss = 19.10303895\n",
      "Iteration 68, loss = 18.84654837\n",
      "Iteration 69, loss = 18.58691601\n",
      "Iteration 70, loss = 18.33429578\n",
      "Iteration 71, loss = 18.08018761\n",
      "Iteration 72, loss = 17.82769493\n",
      "Iteration 73, loss = 17.57510434\n",
      "Iteration 74, loss = 17.32750533\n",
      "Iteration 75, loss = 17.07541594\n",
      "Iteration 76, loss = 16.82765474\n",
      "Iteration 77, loss = 16.58557879\n",
      "Iteration 78, loss = 16.33932148\n",
      "Iteration 79, loss = 16.09920699\n",
      "Iteration 80, loss = 15.85331688\n",
      "Iteration 81, loss = 15.61570587\n",
      "Iteration 82, loss = 15.37481601\n",
      "Iteration 83, loss = 15.13890086\n",
      "Iteration 84, loss = 14.90073651\n",
      "Iteration 85, loss = 14.66878826\n",
      "Iteration 86, loss = 14.43471920\n",
      "Iteration 87, loss = 14.20138415\n",
      "Iteration 88, loss = 13.97446409\n",
      "Iteration 89, loss = 13.74345720\n",
      "Iteration 90, loss = 13.52014529\n",
      "Iteration 91, loss = 13.29516818\n",
      "Iteration 92, loss = 13.07099080\n",
      "Iteration 93, loss = 12.85047039\n",
      "Iteration 94, loss = 12.63224210\n",
      "Iteration 95, loss = 12.41148428\n",
      "Iteration 96, loss = 12.19576121\n",
      "Iteration 97, loss = 11.98216630\n",
      "Iteration 98, loss = 11.77338589\n",
      "Iteration 99, loss = 11.55896287\n",
      "Iteration 100, loss = 11.35073677\n",
      "Iteration 101, loss = 11.14920414\n",
      "Iteration 102, loss = 10.94204424\n",
      "Iteration 103, loss = 10.74210514\n",
      "Iteration 104, loss = 10.54639792\n",
      "Iteration 105, loss = 10.35055740\n",
      "Iteration 106, loss = 10.15415773\n",
      "Iteration 107, loss = 9.96561307\n",
      "Iteration 108, loss = 9.77205128\n",
      "Iteration 109, loss = 9.58771814\n",
      "Iteration 110, loss = 9.40433641\n",
      "Iteration 111, loss = 9.22126943\n",
      "Iteration 112, loss = 9.04131384\n",
      "Iteration 113, loss = 8.86754224\n",
      "Iteration 114, loss = 8.69268946\n",
      "Iteration 115, loss = 8.51724603\n",
      "Iteration 116, loss = 8.35023439\n",
      "Iteration 117, loss = 8.18225561\n",
      "Iteration 118, loss = 8.01792394\n",
      "Iteration 119, loss = 7.86088215\n",
      "Iteration 120, loss = 7.69748647\n",
      "Iteration 121, loss = 7.54629730\n",
      "Iteration 122, loss = 7.38806203\n",
      "Iteration 123, loss = 7.24393339\n",
      "Iteration 124, loss = 7.09335516\n",
      "Iteration 125, loss = 6.94640124\n",
      "Iteration 126, loss = 6.80063092\n",
      "Iteration 127, loss = 6.66218535\n",
      "Iteration 128, loss = 6.52849313\n",
      "Iteration 129, loss = 6.39242355\n",
      "Iteration 130, loss = 6.26016501\n",
      "Iteration 131, loss = 6.13203190\n",
      "Iteration 132, loss = 6.00784512\n",
      "Iteration 133, loss = 5.88892577\n",
      "Iteration 134, loss = 5.76466295\n",
      "Iteration 135, loss = 5.64599583\n",
      "Iteration 136, loss = 5.52872358\n",
      "Iteration 137, loss = 5.41233081\n",
      "Iteration 138, loss = 5.30772088\n",
      "Iteration 139, loss = 5.19899730\n",
      "Iteration 140, loss = 5.08990539\n",
      "Iteration 141, loss = 4.99429838\n",
      "Iteration 142, loss = 4.88896629\n",
      "Iteration 143, loss = 4.79270127\n",
      "Iteration 144, loss = 4.70297049\n",
      "Iteration 145, loss = 4.60451310\n",
      "Iteration 146, loss = 4.51020954\n",
      "Iteration 147, loss = 4.42663628\n",
      "Iteration 148, loss = 4.34113517\n",
      "Iteration 149, loss = 4.25168682\n",
      "Iteration 150, loss = 4.17104179\n",
      "Iteration 151, loss = 4.09135275\n",
      "Iteration 152, loss = 4.00984328\n",
      "Iteration 153, loss = 3.93748210\n",
      "Iteration 154, loss = 3.86178963\n",
      "Iteration 155, loss = 3.79215618\n",
      "Iteration 156, loss = 3.72030772\n",
      "Iteration 157, loss = 3.64912072\n",
      "Iteration 158, loss = 3.58319644\n",
      "Iteration 159, loss = 3.51351447\n",
      "Iteration 160, loss = 3.45204764\n",
      "Iteration 161, loss = 3.39344761\n",
      "Iteration 162, loss = 3.32722346\n",
      "Iteration 163, loss = 3.27078207\n",
      "Iteration 164, loss = 3.21239270\n",
      "Iteration 165, loss = 3.15110117\n",
      "Iteration 166, loss = 3.09842853\n",
      "Iteration 167, loss = 3.04320523\n",
      "Iteration 168, loss = 2.98955240\n",
      "Iteration 169, loss = 2.94224952\n",
      "Iteration 170, loss = 2.89082467\n",
      "Iteration 171, loss = 2.83619376\n",
      "Iteration 172, loss = 2.78752851\n",
      "Iteration 173, loss = 2.73922112\n",
      "Iteration 174, loss = 2.69713375\n",
      "Iteration 175, loss = 2.65012151\n",
      "Iteration 176, loss = 2.60322091\n",
      "Iteration 177, loss = 2.56076018\n",
      "Iteration 178, loss = 2.52047034\n",
      "Iteration 179, loss = 2.47372047\n",
      "Iteration 180, loss = 2.43327541\n",
      "Iteration 181, loss = 2.39553774\n",
      "Iteration 182, loss = 2.36185627\n",
      "Iteration 183, loss = 2.31470403\n",
      "Iteration 184, loss = 2.28215807\n",
      "Iteration 185, loss = 2.24452347\n",
      "Iteration 186, loss = 2.20794769\n",
      "Iteration 187, loss = 2.17512893\n",
      "Iteration 188, loss = 2.14026533\n",
      "Iteration 189, loss = 2.10481102\n",
      "Iteration 190, loss = 2.07107839\n",
      "Iteration 191, loss = 2.04253175\n",
      "Iteration 192, loss = 2.01153843\n",
      "Iteration 193, loss = 1.98010268\n",
      "Iteration 194, loss = 1.94914407\n",
      "Iteration 195, loss = 1.91785970\n",
      "Iteration 196, loss = 1.89227140\n",
      "Iteration 197, loss = 1.86145801\n",
      "Iteration 198, loss = 1.83719520\n",
      "Iteration 199, loss = 1.80873206\n",
      "Iteration 200, loss = 1.78260038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# Training takes ~12 seconds per epoch/iteration\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128,),\n",
    "    activation=\"logistic\",\n",
    "    verbose=True,\n",
    "    random_state=SEED\n",
    ")\n",
    "mlp = mlp.fit(X_train_standardized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRAP on training data: 0.9879\n"
     ]
    }
   ],
   "source": [
    "pred_train = mlp.predict_proba(X_train_standardized)\n",
    "score = LRAP(y_train, pred_train)\n",
    "print(f\"LRAP on training data: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRAP on validation data: 0.5215\n"
     ]
    }
   ],
   "source": [
    "pred_valid = mlp.predict_proba(X_valid_standardized)\n",
    "score = LRAP(y_valid, pred_valid)\n",
    "print(f\"LRAP on validation data: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
