{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- 90% training data, 10% validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13959 examples in training set, and 1552 in validation.\n"
     ]
    }
   ],
   "source": [
    "# Needed to import custom code from other directories\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models import ModifiedMLPClassifier\n",
    "from utils import LRAP\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "NUM_FEATURES = 5000\n",
    "NUM_CLASSES = 3993\n",
    "\n",
    "X_train = pd.read_csv(\"../data/expanded/train_features.csv\", names=range(NUM_FEATURES))\n",
    "y_train = pd.read_csv(\"../data/expanded/train_labels.csv\", names=range(NUM_CLASSES))\n",
    "\n",
    "X_valid = pd.read_csv(\"../data/expanded/valid_features.csv\", names=range(NUM_FEATURES))\n",
    "y_valid = pd.read_csv(\"../data/expanded/valid_labels.csv\", names=range(NUM_CLASSES))\n",
    "\n",
    "print(f\"{X_train.shape[0]} examples in training set, and {X_valid.shape[0]} in validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data\n",
    "\n",
    "- Subtracting mean and dividing by variance.\n",
    "- Statistics of training data is used for the validation data too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "standardize = StandardScaler()\n",
    "standardize = standardize.fit(X_train)\n",
    "X_train_transformed = standardize.transform(X_train)\n",
    "X_valid_transformed = standardize.transform(X_valid)\n",
    "\n",
    "# X_train_transformed = X_train\n",
    "# X_valid_transformed = X_valid\n",
    "# \n",
    "# normalize = MinMaxScaler()\n",
    "# normalize = normalize.fit(X_train_transformed)\n",
    "# X_train_transformed = normalize.transform(X_train_transformed)\n",
    "# X_valid_transformed = normalize.transform(X_valid_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "While training the multi-layer perceptron, the training and validation loss are reported in each iteration (or epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training loss = 366.20741560\n",
      "Iteration 2, training loss = 50.11869569\n",
      "Iteration 3, training loss = 41.02829895\n",
      "Iteration 4, training loss = 36.70106804\n",
      "Iteration 5, training loss = 33.92558299\n",
      "Iteration 6, training loss = 31.80996072\n",
      "Iteration 7, training loss = 30.05609964\n",
      "Iteration 8, training loss = 28.55781174\n",
      "Iteration 9, training loss = 27.23589630\n",
      "Iteration 10, training loss = 26.07287757\n",
      "Iteration 11, training loss = 25.02412659\n",
      "Iteration 12, training loss = 24.05431919\n",
      "Iteration 13, training loss = 23.18079283\n",
      "Iteration 14, training loss = 22.37021015\n",
      "Iteration 15, training loss = 21.62255159\n",
      "Iteration 16, training loss = 20.93691998\n",
      "Iteration 17, training loss = 20.29477947\n",
      "Iteration 18, training loss = 19.70121674\n",
      "Iteration 19, training loss = 19.15362322\n",
      "Iteration 20, training loss = 18.65309881\n",
      "Iteration 21, training loss = 18.18355050\n",
      "Iteration 22, training loss = 17.74745905\n",
      "Iteration 23, training loss = 17.35022182\n",
      "Iteration 24, training loss = 16.95727908\n",
      "Iteration 25, training loss = 16.60948670\n",
      "Iteration 26, training loss = 16.26729480\n",
      "Iteration 27, training loss = 15.95504831\n",
      "Iteration 28, training loss = 15.66162889\n",
      "Iteration 29, training loss = 15.39361344\n",
      "Iteration 30, training loss = 15.14145729\n",
      "Iteration 31, training loss = 14.91594628\n",
      "Iteration 32, training loss = 14.69477786\n",
      "Iteration 33, training loss = 14.47910760\n",
      "Iteration 34, training loss = 14.28851257\n",
      "Iteration 35, training loss = 14.10645219\n",
      "Iteration 36, training loss = 13.95499024\n",
      "Iteration 37, training loss = 13.80429984\n",
      "Iteration 38, training loss = 13.67782325\n",
      "Iteration 39, training loss = 13.56925247\n",
      "Iteration 40, training loss = inf\n",
      "Iteration 41, training loss = 13.34221248\n",
      "Iteration 42, training loss = inf\n",
      "Iteration 43, training loss = inf\n",
      "Iteration 44, training loss = inf\n",
      "Iteration 45, training loss = 12.87337439\n",
      "Iteration 46, training loss = 12.78186638\n",
      "Iteration 47, training loss = 12.68959054\n",
      "Iteration 48, training loss = 12.60986602\n",
      "Iteration 49, training loss = 12.56536938\n",
      "Iteration 50, training loss = 12.49449341\n",
      "Iteration 51, training loss = 12.43968316\n",
      "Iteration 52, training loss = 12.40999846\n",
      "Iteration 53, training loss = 12.33891580\n",
      "Iteration 54, training loss = 12.28269512\n",
      "Iteration 55, training loss = 12.22242169\n",
      "Iteration 56, training loss = 12.17031542\n",
      "Iteration 57, training loss = 12.13532071\n",
      "Iteration 58, training loss = 12.09340247\n",
      "Iteration 59, training loss = inf\n",
      "Iteration 60, training loss = 12.06410921\n",
      "Iteration 61, training loss = inf\n",
      "Iteration 62, training loss = inf\n",
      "Iteration 63, training loss = inf\n",
      "Iteration 64, training loss = inf\n",
      "Iteration 65, training loss = 11.96627237\n",
      "Iteration 66, training loss = 11.92175133\n",
      "Iteration 67, training loss = 11.88476299\n",
      "Iteration 68, training loss = inf\n",
      "Iteration 69, training loss = inf\n",
      "Iteration 70, training loss = inf\n",
      "Iteration 71, training loss = 11.75876870\n",
      "Iteration 72, training loss = 11.72326381\n",
      "Iteration 73, training loss = 11.71505518\n",
      "Iteration 74, training loss = 11.67456005\n",
      "Iteration 75, training loss = 11.67909653\n",
      "Iteration 76, training loss = 11.67260980\n",
      "Iteration 77, training loss = 11.65317422\n",
      "Iteration 78, training loss = inf\n",
      "Iteration 79, training loss = 11.66305678\n",
      "Iteration 80, training loss = inf\n",
      "Iteration 81, training loss = inf\n",
      "Iteration 82, training loss = inf\n",
      "Iteration 83, training loss = inf\n",
      "Iteration 84, training loss = 11.90304917\n",
      "Iteration 85, training loss = 11.81894959\n",
      "Iteration 86, training loss = 11.74299045\n",
      "Iteration 87, training loss = 11.69252011\n",
      "Iteration 88, training loss = 11.59590994\n",
      "Iteration 89, training loss = 11.56873811\n",
      "Iteration 90, training loss = 11.53749560\n",
      "Iteration 91, training loss = 11.50433204\n",
      "Iteration 92, training loss = 11.48009106\n",
      "Iteration 93, training loss = inf\n",
      "Iteration 94, training loss = 11.46503187\n",
      "Iteration 95, training loss = 11.46901086\n",
      "Iteration 96, training loss = 11.46765605\n",
      "Iteration 97, training loss = inf\n",
      "Iteration 98, training loss = inf\n",
      "Iteration 99, training loss = 11.68545892\n",
      "Iteration 100, training loss = 11.70538340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../code\\models.py:248: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "# Training takes ~10 seconds per iteration\n",
    "mlp = ModifiedMLPClassifier(\n",
    "    hidden_layer_sizes=(256,),\n",
    "    max_iter=100,\n",
    "    alpha=0.5,\n",
    "    verbose=True,\n",
    "    random_state=SEED,\n",
    "    # custom_validation_data=(X_valid_transformed, y_valid)\n",
    ")\n",
    "mlp = mlp.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRAP on training data: 0.9251\n"
     ]
    }
   ],
   "source": [
    "pred_train = mlp.predict_proba(X_train_transformed)\n",
    "score_train = LRAP(y_train, pred_train)\n",
    "print(f\"LRAP on training data: {score_train:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRAP on validation data: 0.5174\n"
     ]
    }
   ],
   "source": [
    "pred_valid = mlp.predict_proba(X_valid_transformed)\n",
    "score_valid = LRAP(y_valid, pred_valid)\n",
    "print(f\"LRAP on validation data: {score_valid:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 123\n",
    "sample_pred = mlp.predict(X_valid_transformed[[sample_idx]])[0]\n",
    "actual = y_valid.loc[sample_idx].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([342, 418, 637, 682, 906, 992], dtype=int64),)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 128,  342,  637,  992, 1185, 1722], dtype=int64),)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(sample_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
